{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTTjklfIAsMLFish13R21m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/212-Sobolev/Math_Prac-1-sem-/blob/hometask_7/Hometask_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача: реализовать и запустить модель LDA и Gibbs Sampling с числов тегов 20. Вывести топ-10 слов по каждому тегу. Соотнести полученные теги с тегами из датасета. Добейтесь того, чтобы хотя бы несколько тем были явно интерпретируемы, например, как в примерах ниже.\n",
        "\n",
        "Примеры топ-10 слов из некотрых тегов, которые получаются после применения LDA:\n",
        "\n",
        "['god', 'jesus', 'believe', 'life', 'bible', 'christian', 'world', 'church', 'word', 'people'] - эта группа явно соотносится с soc.religion.christian\n",
        "['drive', 'card', 'hard', 'bit', 'disk', 'scsi', 'memory', 'speed', 'mac', 'video'] - эту группу можно соотнести с темами 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware'\n",
        "['game', 'games', 'hockey', 'league', 'play', 'players', 'season', 'team', 'teams', 'win'] - тема rec.sport.hockey\n",
        "Советы:\n",
        "\n",
        "модель будет сходится лучше и быстрее, если уменьшить размер словаря за счет отсеивания общеупотребительных слов и редких слов. Управлять размером словаря можно с помощью параметров min_df (отсеивает слова по минимальной частоте встречаемости) и max_df (отсеивает слова по максимальной частоте встречаемости) в CountVectorizer.\n",
        "параметры  α ,  β  можно, для начала, положить единицами\n",
        "после 100 итераций можно ожидать хорошего распределения по темам. Если этого не происходит и в темах мешинина - проверяйте код и оптимизируйте словарь\n",
        "на примере третьей темы видно, что у нас встречаются разные формы одного и того же слова. С помощью процедур stemming и lemmatization можно привести слова к общей форме и объединить близкие по значению"
      ],
      "metadata": {
        "id": "h3eqWMpZ7JBI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3_e3K207DEz",
        "outputId": "f05ec6e9-331e-40fd-af54-18ebc43441ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Топ-10 слов по темам:\n",
            "Тема 1: ['god', 'one', 'wa', 'jesus', 'christian', 'doe', 'ha', 'say', 'believe', 'bible']\n",
            "Тема 2: ['wa', 'year', 'game', 'good', 'ha', 'last', 'player', 'team', 'would', 'think']\n",
            "Тема 3: ['car', 'bike', 'get', 'one', 'engine', 'good', 'much', 'go', 'mile', 'front']\n",
            "Тема 4: ['space', 'wa', 'launch', 'nasa', 'system', 'satellite', 'mission', 'earth', 'data', 'ha']\n",
            "Тема 5: ['key', 'chip', 'encryption', 'system', 'use', 'clipper', 'government', 'phone', 'one', 'ha']\n",
            "Тема 6: ['wa', 'one', 'said', 'people', 'say', 'could', 'know', 'would', 'time', 'go']\n",
            "Тема 7: ['one', 'price', 'sale', 'used', 'power', 'use', 'good', 'offer', 'box', 'wire']\n",
            "Тема 8: ['drive', 'card', 'disk', 'system', 'problem', 'driver', 'use', 'mac', 'hard', 'memory']\n",
            "Тема 9: ['game', 'team', 'play', 'hockey', 'period', 'pt', 'new', 'la', 'wa', 'league']\n",
            "Тема 10: ['university', 'study', 'research', 'drug', 'health', 'use', 'center', 'new', 'number', 'medical']\n",
            "Тема 11: ['would', 'know', 'anyone', 'thanks', 'please', 'doe', 'help', 'could', 'like', 'looking']\n",
            "Тема 12: ['problem', 'time', 'one', 'ha', 'get', 'would', 'also', 'much', 'cause', 'like']\n",
            "Тема 13: ['wa', 'ha', 'president', 'think', 'know', 'going', 'job', 'people', 'would', 'stephanopoulos']\n",
            "Тема 14: ['armenian', 'wa', 'jew', 'israel', 'people', 'war', 'turkish', 'israeli', 'government', 'ha']\n",
            "Тема 15: ['would', 'think', 'like', 'people', 'thing', 'one', 'want', 'get', 'know', 'make']\n",
            "Тема 16: ['max', 'db', 'bhj', 'giz', 'ah', 'wm', 'gk', 'air', 'bj', 'mr']\n",
            "Тема 17: ['file', 'window', 'program', 'entry', 'use', 'line', 'output', 'set', 'using', 'error']\n",
            "Тема 18: ['gun', 'right', 'law', 'state', 'would', 'wa', 'government', 'people', 'weapon', 'crime']\n",
            "Тема 19: ['list', 'information', 'group', 'posting', 'send', 'address', 'mail', 'article', 'internet', 'email']\n",
            "Тема 20: ['image', 'available', 'version', 'file', 'software', 'also', 'program', 'system', 'use', 'data']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Загрузка NLTK данных (если не загружено)\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "    nltk.data.find(\"corpora/wordnet\")\n",
        "    nltk.data.find(\"corpora/stopwords\")\n",
        "    nltk.data.find(\"tokenizers/punkt_tab\")\n",
        "except LookupError:\n",
        "    nltk.download(\"punkt\")\n",
        "    nltk.download(\"wordnet\")\n",
        "    nltk.download(\"stopwords\")\n",
        "    nltk.download(\"punkt_tab\")\n",
        "\n",
        "\n",
        "# Предобработка текста\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [\n",
        "        lemmatizer.lemmatize(token) for token in tokens if token.isalpha()\n",
        "    ]\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def lda_gibbs_sampling(\n",
        "    documents, vocab_size, num_topics, num_iterations, alpha, beta\n",
        "):\n",
        "    \"\"\"\n",
        "    Реализация LDA с Gibbs Sampling.\n",
        "    \"\"\"\n",
        "    doc_topic_counts = np.zeros((len(documents), num_topics), dtype=int)\n",
        "    topic_word_counts = np.zeros((num_topics, vocab_size), dtype=int)\n",
        "    topic_counts = np.zeros(num_topics, dtype=int)\n",
        "    doc_word_indices = []\n",
        "\n",
        "    for doc_id, doc in enumerate(documents):\n",
        "        words = doc.split()\n",
        "        doc_indices = []\n",
        "        for word in words:\n",
        "           try:\n",
        "             word_id = word_to_id[word]\n",
        "             doc_indices.append(word_id)\n",
        "           except:\n",
        "             pass\n",
        "        doc_word_indices.append(doc_indices)\n",
        "\n",
        "    # Инициализация\n",
        "    doc_topic_assignments = []\n",
        "    for doc_index, doc in enumerate(doc_word_indices):\n",
        "      topic_assigments = []\n",
        "      for word_id in doc:\n",
        "          topic = np.random.randint(num_topics)\n",
        "          topic_assigments.append(topic)\n",
        "          doc_topic_counts[doc_index][topic] += 1\n",
        "          topic_word_counts[topic][word_id] += 1\n",
        "          topic_counts[topic] += 1\n",
        "      doc_topic_assignments.append(topic_assigments)\n",
        "\n",
        "    # Gibbs Sampling\n",
        "    for it in range(num_iterations):\n",
        "        for doc_id, doc in enumerate(doc_word_indices):\n",
        "          for word_index, word_id in enumerate(doc):\n",
        "            topic = doc_topic_assignments[doc_id][word_index]\n",
        "\n",
        "            doc_topic_counts[doc_id][topic] -= 1\n",
        "            topic_word_counts[topic][word_id] -= 1\n",
        "            topic_counts[topic] -= 1\n",
        "\n",
        "            prob = np.zeros(num_topics)\n",
        "            for k in range(num_topics):\n",
        "              prob[k] = (doc_topic_counts[doc_id][k] + alpha) * (topic_word_counts[k][word_id] + beta) / (topic_counts[k] + vocab_size * beta)\n",
        "\n",
        "            prob /= prob.sum()\n",
        "            new_topic = np.random.choice(num_topics, p=prob)\n",
        "\n",
        "            doc_topic_assignments[doc_id][word_index] = new_topic\n",
        "            doc_topic_counts[doc_id][new_topic] += 1\n",
        "            topic_word_counts[new_topic][word_id] += 1\n",
        "            topic_counts[new_topic] += 1\n",
        "\n",
        "    return topic_word_counts\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Загрузка датасета\n",
        "    newsgroups = fetch_20newsgroups(\n",
        "        remove=(\"headers\", \"footers\", \"quotes\"), random_state=42\n",
        "    )\n",
        "    documents = newsgroups.data\n",
        "\n",
        "    # Предобработка текста\n",
        "    processed_documents = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "    # Создание матрицы \"документ-слово\"\n",
        "    vectorizer = CountVectorizer(min_df=5, max_df=0.95, max_features = 5000)\n",
        "    X = vectorizer.fit_transform(processed_documents)\n",
        "    vocab = vectorizer.get_feature_names_out()\n",
        "    word_to_id = {word: i for i, word in enumerate(vocab)}\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Параметры LDA\n",
        "    num_topics = 20\n",
        "    num_iterations = 100\n",
        "    alpha = 1  # Гиперпараметр Dirichlet prior для тем\n",
        "    beta = 1  # Гиперпараметр Dirichlet prior для слов\n",
        "\n",
        "    # Применение LDA с Gibbs Sampling\n",
        "    topic_word_counts = lda_gibbs_sampling(\n",
        "        processed_documents, vocab_size, num_topics, num_iterations, alpha, beta\n",
        "    )\n",
        "\n",
        "    # Вывод результатов\n",
        "    print(\"Топ-10 слов по темам:\")\n",
        "    for topic_id in range(num_topics):\n",
        "        topic_words = np.argsort(topic_word_counts[topic_id])[::-1][:10]\n",
        "        top_words = [vocab[word_index] for word_index in topic_words]\n",
        "        print(f\"Тема {topic_id+1}: {top_words}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mRATaTAF7ISW"
      }
    }
  ]
}